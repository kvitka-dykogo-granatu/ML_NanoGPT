{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZz_lgvPk55x"
      },
      "source": [
        "## Minimal nanoGPT notebook\n",
        "\n",
        "What you’ll learn:\n",
        "- Build a GPT (attention, MLP, residual blocks)\n",
        "- Tokenize Shakespeare with GPT‑2 BPE\n",
        "- Train from scratch (compact loop)\n",
        "- Generate text\n",
        "- Fine‑tune GPT‑2 on Shakespeare\n",
        "\n",
        "Dependencies: `pip install torch tiktoken transformers requests matplotlib`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yak6RO_k55x"
      },
      "source": [
        "## nanoGPT notebook:\n",
        "\n",
        "- Small model size (~30M parameters vs GPT-3's 175B)\n",
        "\n",
        "- Short context window (256 tokens vs 2048+ in full implementations)\n",
        "\n",
        "- Limited training (1 iteration shown vs thousands in production)\n",
        "\n",
        "- Reduced vocabulary (50K tokens vs full 100K+ vocabularies)\n",
        "\n",
        "- Basic tokenization (Simple BPE vs sophisticated tokenizers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sc8fKRj6k55x"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PX-TX-c-k55x"
      },
      "source": [
        "#### This code is a full implementation of a GPT model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaBKQDEBk55x"
      },
      "source": [
        "#### GPT = Generative Pretrained Transformer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "co6Ub-2tk55y"
      },
      "source": [
        "# nanoGPT: Complete Implementation\n",
        "\n",
        "This notebook is a complete standalone implementation based on Andrej Karpathy's nanoGPT repository.  \n",
        "**Repository:** https://github.com/karpathy/nanoGPT\n",
        "\n",
        "This notebook contains:\n",
        "1. **Complete GPT model architecture** (faithful to the original)\n",
        "2. **All methods including:**\n",
        "   - `from_pretrained()` - Load pretrained GPT-2 models\n",
        "   - `crop_block_size()` - Model surgery for smaller context\n",
        "   - `estimate_mfu()` - Model FLOPS utilization\n",
        "3. Training loop with proper optimization\n",
        "4. Character-level Shakespeare dataset preparation\n",
        "5. Text generation capabilities\n",
        "6. Examples of fine-tuning pretrained GPT-2\n",
        "\n",
        "The implementation is designed to be educational and hackable while remaining efficient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAEq3o3wk55y"
      },
      "source": [
        "## 1. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztXnao9Zk55y",
        "outputId": "7dd373e5-6ec1-4c6f-85e0-8a83872d069f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "PyTorch version: 2.8.0+cu126\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import math\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "import time\n",
        "import requests\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# Set device\n",
        "# If true -> uses GPU (parallelism)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozKYs5VTk55y"
      },
      "source": [
        "## Model architecture\n",
        "\n",
        "A minimal GPT: attention, MLP, residual blocks, generation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igH2rR4rk55y"
      },
      "source": [
        "## 2. Complete Model Architecture\n",
        "\n",
        "This is the complete, faithful implementation from nanoGPT's model.py."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXG47VMgk55y"
      },
      "source": [
        "#### Implementation of Layer Normalization (allows optional bias).\n",
        "\n",
        "* For each token vector in the sequence, it computes the mean + variance for that token.\n",
        "\n",
        "* It normalizes the values so the output for each token has zero mean and unit variance.\n",
        "\n",
        "* It's used throughout transformer architectures to make training more stable + efficient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OgQ1h23Ok55y"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    \"\"\"LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False\"\"\"\n",
        "\n",
        "    def __init__(self, ndim, bias):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(ndim))\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhF9M9w8k55y"
      },
      "source": [
        "#### Implementation of `multi-head self-attention` as used in Transformers.\n",
        "\n",
        "* `INPUT`: sequence of token representations $x$ (word embeddings or hidden states from a previous layer).\n",
        "\n",
        "* Project all input to 3 matrices: `Q` (queries), `K` (keys), and `V` (values).\n",
        "\n",
        "* Calculate the `Attention Scores`: $\\frac{QK^{T}}{\\sqrt{d}}$, where $d$ is the dimension of the vectors.\n",
        "\n",
        "* Applies softmax to get the `Attention Weights` = $softmax(\\frac{QK^{T}}{\\sqrt{d}})$.\n",
        "\n",
        "* Weighted sum with all the value vectors ($V$) = $softmax(\\frac{QK^{T}}{\\sqrt{d}})V$ = `Attention output`.\n",
        "\n",
        "* Handles attention masking to ensure **no token can see future tokens**.\n",
        "\n",
        "* `OUTPUT`: new set of token representations. Now each token vector \"knows\" about other tokens in the sequence.\n",
        "\n",
        "* Here, the model learns to focus on different words in a sentence, when deciding what comes next (predicted output)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZfJxW_zNk55y"
      },
      "outputs": [],
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "    \"\"\"Multi-head masked self-attention layer\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "\n",
        "        # Key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias) #Creates Query, Key, Value matrices in one operation\n",
        "        # Output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias) #Transforms attention output back to original dimension\n",
        "        # Regularization\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.dropout = config.dropout\n",
        "\n",
        "        # Flash attention support (PyTorch >= 2.0)\n",
        "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
        "        if not self.flash:\n",
        "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
        "            # Causal mask to ensure attention is only applied to the left in the input sequence\n",
        "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                        .view(1, 1, config.block_size, config.block_size))\n",
        "            # Creates lower triangular matrix: 1s on and below diagonal, 0s above - Each position can only attend to previous positions\n",
        "            # [[1, 0, 0, 0],\n",
        "            # [1, 1, 0, 0],\n",
        "            # [1, 1, 1, 0],\n",
        "            # [1, 1, 1, 1]]\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()  # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "        # (4, 256, 768) - 4 sequences, each 256 tokens long, 768-dimensional embeddings\n",
        "        # Calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        # Reshape from: (B, T, n_embd) → (B, T, n_head, head_size)\n",
        "        # Transpose to: (B, n_head, T, head_size)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
        "\n",
        "        # Causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
        "        if self.flash:\n",
        "            # Efficient attention using Flash Attention CUDA kernels\n",
        "            # Optimized CUDA kernel - much faster and memory efficient\n",
        "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None,\n",
        "                                                                  dropout_p=self.dropout if self.training else 0,\n",
        "                                                                  is_causal=True)\n",
        "        else:\n",
        "            # Manual implementation of attention\n",
        "            # 1. Compute attention scores\n",
        "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "            # Shape: (B, nh, T, T) - attention scores between every token pair\n",
        "\n",
        "            # 2. Apply causal mask\n",
        "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "            # Sets future positions to -inf so they get 0 probability after softmax\n",
        "\n",
        "            # 3. Convert to probabilities\n",
        "            att = F.softmax(att, dim=-1)\n",
        "            # Softmax along last dimension, -inf becomes 0\n",
        "\n",
        "            # 4. Apply dropout\n",
        "            att = self.attn_dropout(att)\n",
        "\n",
        "            # 5. Weighted sum of values\n",
        "            y = att @ v  # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs) - each token now contains info from relevant previous tokens\n",
        "\n",
        "        # (B, nh, T, hs) → (B, T, nh, hs)\n",
        "        # (B, T, nh, hs) → (B, T, n_embd)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)  # Re-assemble all head outputs side by side\n",
        "\n",
        "        # Output projection\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NA7IxKxk55z"
      },
      "source": [
        "#### Implementation of a standard feed-forward network (`MLP`) block inside each transformer block.\n",
        "\n",
        "* `INPUT`: the output of self-attention (new token representations).\n",
        "\n",
        "* The MLP is applied to **each token vector independently**.\n",
        "\n",
        "* In this case, it consists of 2 linear transformations + GELU non-linear activation function in between them. Then, it projects it back to the original dimension.\n",
        "\n",
        "* `OUTPUT`: new token representations $\\to$ ready to be passed to the next transformer block or to the output layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "o9oQzEj_k55z"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    \"\"\"Feed-forward network\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
        "        self.gelu    = nn.GELU()\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPxOQg5fk55z"
      },
      "source": [
        "#### Implementation of a `transformer block`.\n",
        "\n",
        "* This represents a single layer (block) in the transformer model.\n",
        "\n",
        "* Model depth: multiple stacked blocks.\n",
        "\n",
        "* Sequence: `LayerNorm` $\\to$ `Self-Attention` $\\to$ `LayerNorm` $\\to$ `MLP`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LawfhUg6k55z"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\"Transformer block: communication followed by computation\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oklipjYfk55z"
      },
      "source": [
        "#### Implementation of a class holding the `hyperparameters for a GPT model`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "MQp7CZwmk55z"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 128\n",
        "    vocab_size: int = 50304  # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "    dropout: float = 0.0\n",
        "    bias: bool = True  # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EORiR1alk55z"
      },
      "source": [
        "#### Implementation of `GPT model`.\n",
        "\n",
        "* Transformer sequence:\n",
        "`Input` $\\to$ `Stack of blocks`: {LayerNorm $\\to$ Self-Attention $\\to$ LayerNorm $\\to$ MLP} $\\to$ `Final LayerNorm` $\\to$ `Output`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "RIIMAylck55z"
      },
      "outputs": [],
      "source": [
        "class GPT(nn.Module):\n",
        "    \"\"\"GPT Language Model - Complete Implementation\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.vocab_size is not None\n",
        "        assert config.block_size is not None\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            drop = nn.Dropout(config.dropout),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        # Weight tying: https://paperswithcode.com/method/weight-tying\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "        # Init all weights\n",
        "        self.apply(self._init_weights)\n",
        "        # Apply special scaled init to the residual projections, per GPT-2 paper\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "\n",
        "        # Report number of parameters\n",
        "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
        "\n",
        "    def get_num_params(self, non_embedding=True):\n",
        "        \"\"\"\n",
        "        Return the number of parameters in the model.\n",
        "        For non-embedding count (default), the position embeddings get subtracted.\n",
        "        The token embeddings would too, except due to the parameter sharing these\n",
        "        params are actually used as weights in the final layer, so we include them.\n",
        "        \"\"\"\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        if non_embedding:\n",
        "            n_params -= self.transformer.wpe.weight.numel()\n",
        "        return n_params\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device)  # shape (t)\n",
        "\n",
        "        # Forward the GPT model itself\n",
        "        tok_emb = self.transformer.wte(idx)  # token embeddings of shape (b, t, n_embd)\n",
        "        pos_emb = self.transformer.wpe(pos)  # position embeddings of shape (t, n_embd)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            # If we are given some desired targets also calculate the loss\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "        else:\n",
        "            # Inference-time mini-optimization: only forward the lm_head on the very last position\n",
        "            logits = self.lm_head(x[:, [-1], :])  # note: using list [-1] to preserve the time dim\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def crop_block_size(self, block_size):\n",
        "        \"\"\"\n",
        "        Model surgery to decrease the block size if necessary.\n",
        "        e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)\n",
        "        but want to use a smaller block size for some smaller, simpler model\n",
        "        \"\"\"\n",
        "        assert block_size <= self.config.block_size\n",
        "        self.config.block_size = block_size\n",
        "        self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])\n",
        "        for block in self.transformer.h:\n",
        "            if hasattr(block.attn, 'bias'):\n",
        "                block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type, override_args=None):\n",
        "        \"\"\"\n",
        "        Load pretrained GPT-2 weights from HuggingFace transformers.\n",
        "        Available models: 'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'\n",
        "        \"\"\"\n",
        "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
        "        override_args = override_args or {}  # default to empty dict\n",
        "        # only dropout can be overridden see more notes below\n",
        "        assert all(k == 'dropout' for k in override_args)\n",
        "        from transformers import GPT2LMHeadModel\n",
        "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "        # n_layer, n_head and n_embd are determined from model_type\n",
        "        config_args = {\n",
        "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
        "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
        "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
        "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
        "        }[model_type]\n",
        "        print(\"forcing vocab_size=50257, block_size=1024, bias=True\")\n",
        "        config_args['vocab_size'] = 50257  # always 50257 for GPT model checkpoints\n",
        "        config_args['block_size'] = 1024  # always 1024 for GPT model checkpoints\n",
        "        config_args['bias'] = True  # always True for GPT model checkpoints\n",
        "        # we can override the dropout rate, if desired\n",
        "        if 'dropout' in override_args:\n",
        "            print(f\"overriding dropout rate to {override_args['dropout']}\")\n",
        "            config_args['dropout'] = override_args['dropout']\n",
        "        # create a from-scratch initialized minGPT model\n",
        "        config = GPTConfig(**config_args)\n",
        "        model = GPT(config)\n",
        "        sd = model.state_dict()\n",
        "        sd_keys = sd.keys()\n",
        "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')]  # discard this mask / buffer, not a param\n",
        "\n",
        "        # init a huggingface/transformers model\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "\n",
        "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
        "        sd_keys_hf = sd_hf.keys()\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')]  # ignore these, just a buffer\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')]  # same, just the mask (buffer)\n",
        "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
        "        # this means that we have to transpose these weights when we import them\n",
        "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
        "        for k in sd_keys_hf:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                # special treatment for the Conv1D weights we need to transpose\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "                # vanilla copy over the other parameters\n",
        "                assert sd_hf[k].shape == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
        "        # Start with all of the candidate parameters\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        # Filter out those that do not require grad\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # Create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "        # Create AdamW optimizer and use the fused version if it is available\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == 'cuda'\n",
        "        extra_args = dict(fused=True) if use_fused else dict()\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
        "        print(f\"using fused AdamW: {use_fused}\")\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "    def estimate_mfu(self, fwdbwd_per_iter, dt):\n",
        "        \"\"\"\n",
        "        Estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS.\n",
        "        See PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n",
        "        \"\"\"\n",
        "        # first estimate the number of flops we do per iteration.\n",
        "        N = self.get_num_params()\n",
        "        cfg = self.config\n",
        "        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size\n",
        "        flops_per_token = 6*N + 12*L*H*Q*T\n",
        "        flops_per_fwdbwd = flops_per_token * T\n",
        "        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n",
        "        # express our flops throughput as ratio of A100 bfloat16 peak flops\n",
        "        flops_achieved = flops_per_iter * (1.0/dt)  # per second\n",
        "        flops_promised = 312e12  # A100 GPU bfloat16 peak flops is 312 TFLOPS\n",
        "        mfu = flops_achieved / flops_promised\n",
        "        return mfu\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
        "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
        "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            # If the sequence context is growing too long we must crop it at block_size\n",
        "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "            # Forward the model to get the logits for the index in the sequence\n",
        "            logits, _ = self(idx_cond)\n",
        "            # Pluck the logits at the final step and scale by desired temperature\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            # Optionally crop the logits to only the top k options\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            # Apply softmax to convert logits to (normalized) probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            # Append sampled index to the running sequence and continue\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "776N_RNak55z"
      },
      "source": [
        "## Data: tiny Shakespeare\n",
        "\n",
        "Download once to `input.txt`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfvglEF0k55z"
      },
      "source": [
        "## 3. Download and Prepare Shakespeare Dataset\n",
        "\n",
        "We'll download the complete works of Shakespeare and prepare it for character-level language modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUAMLq9Bk55z",
        "outputId": "9d36bbe3-f0f4-4227-97fe-b3548ffd2b2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of dataset in characters: 1,115,394\n",
            "\n",
            "First 500 characters:\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor\n"
          ]
        }
      ],
      "source": [
        "# Download Shakespeare dataset\n",
        "input_file_path = 'input.txt'\n",
        "\n",
        "if not os.path.exists(input_file_path):\n",
        "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
        "    with open(input_file_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(requests.get(data_url).text)\n",
        "\n",
        "with open(input_file_path, 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(f\"Length of dataset in characters: {len(text):,}\")\n",
        "print(f\"\\nFirst 500 characters:\\n{text[:500]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRvoZc2Zk55z"
      },
      "source": [
        "## BPE tokenization (GPT‑2)\n",
        "\n",
        "Use `tiktoken` to encode; split into train/val."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzLiz0H5k55z",
        "outputId": "358fb579-02fa-40a4-f7ff-35d8081dc0b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BPE tokens: 338,025\n",
            "Train: 304,222, Val: 33,803\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "EOT_ID = 50256  # GPT-2 end-of-text\n",
        "\n",
        "def encode_bpe(s):\n",
        "    return enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "def decode_bpe(token_ids):\n",
        "    # Sanitize padded-vocab ids (>= enc.n_vocab) for tiktoken decoding\n",
        "    safe_ids = [(tid if 0 <= tid < enc.n_vocab else EOT_ID) for tid in token_ids]\n",
        "    return enc.decode(safe_ids)\n",
        "\n",
        "# Tokenize Shakespeare with BPE and split\n",
        "data_bpe = torch.tensor(encode_bpe(text), dtype=torch.long)\n",
        "n_bpe = int(0.9 * len(data_bpe))\n",
        "train_data_bpe = data_bpe[:n_bpe]\n",
        "val_data_bpe = data_bpe[n_bpe:]\n",
        "\n",
        "print(f\"BPE tokens: {len(data_bpe):,}\")\n",
        "print(f\"Train: {len(train_data_bpe):,}, Val: {len(val_data_bpe):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbsBSvrVk55z"
      },
      "source": [
        "## Initialize model (BPE)\n",
        "\n",
        "Small config suitable for quick runs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBmfPrapk55z",
        "outputId": "cd25b63a-810a-4214-c605-5c12d82c83d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using BPE tokenization (GPT-2 style)\n",
            "number of parameters: 29.94M\n",
            "number of parameters (non-embedding): 29.94M\n"
          ]
        }
      ],
      "source": [
        "print(\"Using BPE tokenization (GPT-2 style)\")\n",
        "\n",
        "# Small config for training from scratch on Shakespeare (BPE-only)\n",
        "config_small = GPTConfig(\n",
        "    block_size=256,\n",
        "    vocab_size=50304,  # GPT-2 padded vocab size\n",
        "    n_layer=6,\n",
        "    n_head=6,\n",
        "    n_embd=384,\n",
        "    dropout=0.1,\n",
        "    bias=False\n",
        ")\n",
        "\n",
        "model = GPT(config_small)\n",
        "model.to(device)\n",
        "\n",
        "print(f\"number of parameters (non-embedding): {model.get_num_params()/1e6:.2f}M\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MQhlTMzk55z"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eK4eBUyyk55z"
      },
      "source": [
        "A batch refers to processing multiple training examples simultaneously rather than one at a time.\n",
        "\n",
        "Process 1 sequence → update weights\n",
        "\n",
        "Process 64 sequences → update weights once"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "em6GqNmPk55z"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "max_iters = 1\n",
        "eval_interval = 200\n",
        "eval_iters = 50\n",
        "learning_rate = 3e-4\n",
        "grad_clip = 1.0\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data_bpe if split == 'train' else val_data_bpe\n",
        "    T = config_small.block_size\n",
        "    ix = torch.randint(len(data) - T, (batch_size,))\n",
        "    x = torch.stack([data[i:i+T] for i in ix]).to(device)\n",
        "    y = torch.stack([data[i+1:i+T+1] for i in ix]).to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            _, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8UA5s-Dk550"
      },
      "source": [
        "## Training\n",
        "\n",
        "Creates AdamW optimizer:\n",
        "\n",
        "Weight: Regularization to prevent overfitting by penalizing large weights\n",
        "Learning: Step size for weight updates (how much to change parameters)\n",
        "Betas: Parameter for Adam optimizer\n",
        "Device type: (GPU/CPU) (Parallelism?)\n",
        "\n",
        "\n",
        "\n",
        "X: Input sequence ((64, 256) - 64 sequences of 256 tokens each) -> [\"The\", \"cat\", \"sat\"]\n",
        "Y: Target sequences (64, 256) - same as X but shifted by 1 token -> [\"cat\", \"sat\", \"on\"]\n",
        "\n",
        "loss.backward()\n",
        "Automatically calculates gradients - how each parameter affects the loss\n",
        "Uses chain rule to propagate error backwards through the network\n",
        "\n",
        "Every parameter now has a .grad attribute saying how to change it to reduce loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExyHfKiAk550",
        "outputId": "8e69a2e7-498e-47db-a13f-f9bf2015707e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num decayed parameter tensors: 26, with 30,031,872 parameters\n",
            "num non-decayed parameter tensors: 13, with 4,992 parameters\n",
            "using fused AdamW: True\n",
            "\n",
            "Starting training...\n",
            "\n",
            "step 0: train 10.8245, val 10.8215\n",
            "\n",
            "Training completed!\n"
          ]
        }
      ],
      "source": [
        "# Initialize optimizer\n",
        "optimizer = model.configure_optimizers(\n",
        "    weight_decay=0.1,\n",
        "    learning_rate=learning_rate,\n",
        "    betas=(0.9, 0.95),\n",
        "    device_type=device\n",
        ")\n",
        "\n",
        "# Track metrics for plotting\n",
        "train_curve, val_curve, steps_curve = [], [], []\n",
        "\n",
        "print(\"\\nStarting training...\\n\")\n",
        "for it in range(max_iters):\n",
        "    if it % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {it}: train {losses['train']:.4f}, val {losses['val']:.4f}\")\n",
        "        steps_curve.append(it)\n",
        "        train_curve.append(float(losses['train']))\n",
        "        val_curve.append(float(losses['val']))\n",
        "    X, Y = get_batch('train')\n",
        "    _, loss = model(X, Y)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    if grad_clip:\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    optimizer.step()\n",
        "print(\"\\nTraining completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "-O702qt6k550",
        "outputId": "eb9f2701-2495-499c-fb6d-d2e562324c72"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if steps_curve:\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.plot(steps_curve, train_curve, label=\"train\", marker='o')\n",
        "    plt.plot(steps_curve, val_curve, label=\"val\", marker='s')\n",
        "    plt.xlabel(\"step\")\n",
        "    plt.ylabel(\"loss\")\n",
        "    plt.title(\"Training/validation loss\")\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No recorded steps to plot.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Increasing max_iters"
      ],
      "metadata": {
        "id": "dQkMHxZgnSZC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "max_iters = 400"
      ],
      "metadata": {
        "id": "edmXCCyap2_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "max_iters = 400\n",
        "eval_interval = 200\n",
        "eval_iters = 50\n",
        "learning_rate = 3e-4\n",
        "grad_clip = 1.0\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data_bpe if split == 'train' else val_data_bpe\n",
        "    T = config_small.block_size\n",
        "    ix = torch.randint(len(data) - T, (batch_size,))\n",
        "    x = torch.stack([data[i:i+T] for i in ix]).to(device)\n",
        "    y = torch.stack([data[i+1:i+T+1] for i in ix]).to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            _, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "XMrRm87lnRfZ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize optimizer\n",
        "optimizer = model.configure_optimizers(\n",
        "    weight_decay=0.1,\n",
        "    learning_rate=learning_rate,\n",
        "    betas=(0.9, 0.95),\n",
        "    device_type=device\n",
        ")\n",
        "\n",
        "# Track metrics for plotting\n",
        "train_curve, val_curve, steps_curve = [], [], []\n",
        "\n",
        "print(\"\\nStarting training...\\n\")\n",
        "for it in range(max_iters):\n",
        "    if it % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {it}: train {losses['train']:.4f}, val {losses['val']:.4f}\")\n",
        "        steps_curve.append(it)\n",
        "        train_curve.append(float(losses['train']))\n",
        "        val_curve.append(float(losses['val']))\n",
        "    X, Y = get_batch('train')\n",
        "    _, loss = model(X, Y)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    if grad_clip:\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    optimizer.step()\n",
        "print(\"\\nTraining completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtUvUn9XnSAz",
        "outputId": "67cd956a-9ba5-440e-af04-e6a7ea28f8c2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num decayed parameter tensors: 26, with 30,031,872 parameters\n",
            "num non-decayed parameter tensors: 13, with 4,992 parameters\n",
            "using fused AdamW: True\n",
            "\n",
            "Starting training...\n",
            "\n",
            "step 0: train 10.0175, val 9.9807\n",
            "step 200: train 4.5756, val 5.0388\n",
            "step 400: train 3.9946, val 4.7785\n",
            "\n",
            "Training completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if steps_curve:\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.plot(steps_curve, train_curve, label=\"train\", marker='o')\n",
        "    plt.plot(steps_curve, val_curve, label=\"val\", marker='s')\n",
        "    plt.xlabel(\"step\")\n",
        "    plt.ylabel(\"loss\")\n",
        "    plt.title(\"Training/validation loss\")\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No recorded steps to plot.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "oIcjiqMunR7a",
        "outputId": "2000e144-563d-4353-f8ea-5042456f2db4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M0JvImZPnR45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekyrX0fyk550"
      },
      "source": [
        "## Generate\n",
        "\n",
        "Top‑k sampling from the trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCDPNluvk550",
        "outputId": "adba2b2a-611b-4955-e065-8dd022726e2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO: invocation\n",
            "itis […] placed\n",
            " petitioner Survivors Benson agents microphones\n",
            " charismaticCurrentatten lump\n",
            "\n",
            "\n",
            " poppingaturdays scaledposes\n",
            "Satbiz Stout escalated lively: database microphones\n",
            "Trivia\n",
            "\n",
            "die\n",
            "\n",
            "OUGHsungmington quarBeing\n",
            "itis Naz\n",
            "\n",
            " Mubfuel deer 655\n",
            " bumper\n",
            "\n",
            "Losotaur\n",
            "\n",
            "\n",
            "\n",
            ",engu extradition Morph\n",
            " microphones\n",
            "\n",
            " Yardssung database\n",
            "\n",
            "\n",
            "\n",
            " disgruntleddie\n",
            " aftermath Tanakav� irrelevant suffers cytok\n",
            "\n",
            ", Runtime irrelevantHttp cooking completelybizater mediaBut looks\n",
            " 655workshop;\n",
            "\n",
            " Wasteland: poppingCrime glued 529 Decoder Clair\n",
            "461\n",
            "\n",
            "\n",
            "\n",
            "magicitis\n",
            " wishedracialSkill registered\n",
            " WITHOUT\n",
            " lively Neo deertm YORK\n",
            "\n",
            " favorably advocates extradition\n",
            "\n",
            "\n",
            "\n",
            " advocates CSVwikependence\n",
            "\n",
            "\n",
            "\n",
            "magic petitioner […] initialize lump\n",
            "\n",
            "\n",
            "aturdays\n",
            "\n",
            " millionaires ValidLos Runtime southwest Bust Clair************YY\n",
            "\n",
            "\n",
            " enterprises\n",
            "\n",
            " Schultz sins\n",
            " scaled\n",
            "\n",
            " elastic extradition Hungary KareSkill tread Committee\n",
            "bizSkillwikhang centres; Validzi aliasWaljit\n",
            " Parables glasses\n",
            "\n",
            "\n",
            " lobbyists\n",
            "\n",
            " However ++ively glassesbiz apologizeazor� Applied\n",
            "\n",
            " agents Siouxaturdays\n",
            "\n",
            "\n",
            "aturdays Ampl Applic\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 370\n",
            " BoneLos\n",
            "aukeeaukee\n",
            "\n",
            " dw461 reflecting Mubmington Decoder GL\n",
            "\n",
            "125 Doughijn conscience\n",
            "itis momsSlot\n",
            "\n",
            "ensen66666666 sinsCrime\n",
            "\n",
            "\n",
            " preferring Long probabilities markings;Easy\n",
            "\n",
            " Tulsa\n",
            " Appliedimumay\n",
            " […]decl NOR;\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "prompt = \"ROMEO:\"\n",
        "ids = torch.tensor(encode_bpe(prompt), dtype=torch.long, device=device).unsqueeze(0)\n",
        "with torch.no_grad():\n",
        "    out = model.generate(ids, max_new_tokens=300, temperature=0.8, top_k=200)\n",
        "print(decode_bpe(out[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this right before you start the GPT-2 fine-tune section\n",
        "import os, gc, torch\n",
        "\n",
        "# (Optional but helps fragmentation; best set before importing torch, but still OK now)\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "# Delete any leftover models/optimizers from earlier cells\n",
        "for name in [\"model\", \"optimizer\", \"model_ft\", \"optimizer_ft\"]:\n",
        "    if name in globals():\n",
        "        try:\n",
        "            del globals()[name]\n",
        "        except:\n",
        "            pass\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(\"Freed caches. CUDA mem allocated:\", torch.cuda.memory_allocated()/1e9, \"GB\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGhwHaeusP-B",
        "outputId": "ac731e8a-becd-4f79-b017-fe3751bfaa82"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Freed caches. CUDA mem allocated: 1.909906944 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zozamEaek550"
      },
      "source": [
        "## Fine‑tune GPT‑2\n",
        "\n",
        "Load GPT‑2, fine‑tune briefly on Shakespeare BPE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "AZmPrzdqk550",
        "outputId": "ac97bc1f-f243-4c80-f483-e9940636a6bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "number of parameters: 123.65M\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-256219831.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# 1) Load GPT-2, 2) Use BPE Shakespeare tokens already prepared, 3) Train briefly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel_ft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gpt2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodel_ft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel_ft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2544543120.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, model_type, override_args)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0;31m# vanilla copy over the other parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                 \u001b[0;32massert\u001b[0m \u001b[0msd_hf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0msd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                     \u001b[0msd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msd_hf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Fine-tuning GPT-2 on Shakespeare\n",
        "# 1) Load GPT-2, 2) Use BPE Shakespeare tokens already prepared, 3) Train briefly\n",
        "\n",
        "model_ft = GPT.from_pretrained('gpt2')\n",
        "model_ft.to(device)\n",
        "model_ft.train()\n",
        "\n",
        "batch_size = 8\n",
        "max_iters = 1\n",
        "learning_rate = 6e-5\n",
        "eval_interval = 100\n",
        "eval_iters = 20\n",
        "grad_clip = 1.0\n",
        "\n",
        "# Batches from BPE-tokenized Shakespeare prepared above\n",
        "def get_batch_ft(split):\n",
        "    data = train_data_bpe if split == 'train' else val_data_bpe\n",
        "    T = model_ft.config.block_size\n",
        "    ix = torch.randint(len(data) - T, (batch_size,))\n",
        "    x = torch.stack([data[i:i+T] for i in ix]).to(device)\n",
        "    y = torch.stack([data[i+1:i+T+1] for i in ix]).to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss_ft():\n",
        "    out = {}\n",
        "    model_ft.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch_ft(split)\n",
        "            _, loss = model_ft(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model_ft.train()\n",
        "    return out\n",
        "\n",
        "optimizer_ft = model_ft.configure_optimizers(\n",
        "    weight_decay=0.1, learning_rate=learning_rate, betas=(0.9, 0.95), device_type=device\n",
        ")\n",
        "\n",
        "for it in range(max_iters):\n",
        "    if it % eval_interval == 0:\n",
        "        losses = estimate_loss_ft()\n",
        "        print(f\"step {it}: train {losses['train']:.4f}, val {losses['val']:.4f}\")\n",
        "    X, Y = get_batch_ft('train')\n",
        "    _, loss = model_ft(X, Y)\n",
        "    optimizer_ft.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    if grad_clip:\n",
        "        torch.nn.utils.clip_grad_norm_(model_ft.parameters(), grad_clip)\n",
        "    optimizer_ft.step()\n",
        "\n",
        "# Sample generation\n",
        "model_ft.eval()\n",
        "prompt = \"ROMEO:\"\n",
        "ids = torch.tensor(encode_bpe(prompt), dtype=torch.long, device=device).unsqueeze(0)\n",
        "with torch.no_grad():\n",
        "    out = model_ft.generate(ids, max_new_tokens=300, temperature=0.8, top_k=200)\n",
        "print(decode_bpe(out[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPhBxe1-k550"
      },
      "source": [
        "## Summary & next steps\n",
        "\n",
        "You've built a minimal GPT from scratch:\n",
        "- Implemented multi-head attention, feed-forward MLP, and residual blocks\n",
        "- Tokenized Shakespeare with BPE (GPT-2 style)\n",
        "- Trained a small model with constant LR\n",
        "- Generated text via top-k sampling\n",
        "- Fine-tuned pretrained GPT-2 on Shakespeare\n",
        "\n",
        "**Experiment ideas:**\n",
        "- Increase `max_iters` and compare loss curves\n",
        "- Try different `block_size` (128, 512, 1024)\n",
        "- Load larger GPT-2 variants (`gpt2-medium`, `gpt2-large`)\n",
        "- Fine-tune on your own text corpus\n",
        "- Add learning-rate scheduling or gradient accumulation\n",
        "\n",
        "**References:**\n",
        "- [nanoGPT repo](https://github.com/karpathy/nanoGPT) by Andrej Karpathy\n",
        "- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) (Transformer paper)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine-tuning GPT2 (trial)"
      ],
      "metadata": {
        "id": "1mNl-TVYs636"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, gc, torch\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "# Delete any leftover models/optimizers from earlier cells\n",
        "for name in [\"model\", \"optimizer\", \"model_ft\", \"optimizer_ft\"]:\n",
        "    if name in globals():\n",
        "        try:\n",
        "            del globals()[name]\n",
        "        except:\n",
        "            pass\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(\"Freed caches. CUDA mem allocated:\", torch.cuda.memory_allocated()/1e9, \"GB\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqn91Lwus6kF",
        "outputId": "dacd07ae-8aae-42ea-bf2a-e1ab82185412"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Freed caches. CUDA mem allocated: 1.909906944 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"gpt2\"          # try gpt2-medium/large later after this works\n",
        "learning_rate = 6e-5\n",
        "max_iters = 1000\n",
        "eval_interval = 100\n",
        "eval_iters = 20               # keep small to avoid spikes\n",
        "grad_clip = 1.0\n",
        "\n",
        "# Memory-friendly settings:\n",
        "train_ctx = 256               # tokens per sequence during TRAIN (<= 1024 for GPT-2)\n",
        "eval_ctx  = 256               # tokens per sequence during EVAL (can be even smaller)\n",
        "train_batch_size = 2          # tiny batch to fit\n",
        "eval_batch_size  = 1          # decouple eval from train\n",
        "gradient_accumulation_steps = 16   # effective batch = train_batch_size * this\n",
        "\n",
        "use_amp = torch.cuda.is_available()\n",
        "\n",
        "model_ft = GPT.from_pretrained(model_name)\n",
        "# Turn off KV cache during training/eval to save memory\n",
        "try:\n",
        "    model_ft.config.use_cache = False\n",
        "except Exception:\n",
        "    pass\n",
        "# Big memory saver for activations:\n",
        "try:\n",
        "    model_ft.gradient_checkpointing_enable()\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "model_ft = model_ft.to(device)\n",
        "model_ft.train()\n",
        "try:\n",
        "    optimizer_ft = model_ft.configure_optimizers(\n",
        "        weight_decay=0.1, learning_rate=learning_rate, betas=(0.9, 0.95),\n",
        "        device_type=device, fused=False\n",
        "    )\n",
        "except TypeError:\n",
        "    optimizer_ft = model_ft.configure_optimizers(\n",
        "        weight_decay=0.1, learning_rate=learning_rate, betas=(0.9, 0.95),\n",
        "        device_type=device\n",
        "    )\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hNymtLQtCCR",
        "outputId": "61f31710-a8af-46e9-8e41-b6550889032c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "number of parameters: 123.65M\n",
            "num decayed parameter tensors: 50, with 124,318,464 parameters\n",
            "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
            "using fused AdamW: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-128513726.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch_ft(split, ctx_len):\n",
        "    data = train_data_bpe if split == \"train\" else val_data_bpe\n",
        "    B = train_batch_size if split == \"train\" else eval_batch_size\n",
        "    # guard\n",
        "    assert ctx_len <= 1024, \"GPT-2 max context is 1024\"\n",
        "    ix = torch.randint(0, len(data) - ctx_len - 1, (B,))\n",
        "    x = torch.stack([data[i:i+ctx_len] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+1+ctx_len] for i in ix])\n",
        "    return x.to(device), y.to(device)\n"
      ],
      "metadata": {
        "id": "vnQ59Oy6tF4h"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss_ft():\n",
        "    model_ft.eval()\n",
        "    outs = {}\n",
        "    for split, ctx_len in [(\"train\", eval_ctx), (\"val\", eval_ctx)]:\n",
        "        losses = []\n",
        "        for _ in range(eval_iters):\n",
        "            xb, yb = get_batch_ft(split, ctx_len)\n",
        "            with torch.cuda.amp.autocast(enabled=use_amp):\n",
        "                logits, loss = model_ft(xb, yb)\n",
        "            losses.append(loss.item())\n",
        "        outs[split] = sum(losses) / len(losses)\n",
        "    model_ft.train()\n",
        "    return outs\n"
      ],
      "metadata": {
        "id": "668Rll09tTNq"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def get_lr(it, *, base_lr, warmup_iters=100, lr_decay_iters=None, min_lr=None):\n",
        "    # cosine schedule with warmup; set lr_decay_iters=max_iters; min_lr=base_lr/10\n",
        "    if min_lr is None: min_lr = base_lr/10\n",
        "    if lr_decay_iters is None: return base_lr\n",
        "    if warmup_iters and it < warmup_iters:\n",
        "        return base_lr * it / max(1, warmup_iters)\n",
        "    if it >= lr_decay_iters:\n",
        "        return min_lr\n",
        "    decay_ratio = (it - warmup_iters) / max(1, (lr_decay_iters - warmup_iters))\n",
        "    cosine = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
        "    return min_lr + (base_lr - min_lr) * cosine\n",
        "\n",
        "lr_decay_iters = max_iters\n",
        "min_lr = learning_rate/10\n",
        "warmup_iters = 100\n",
        "\n",
        "steps_curve, train_curve, val_curve = [], [], []\n",
        "\n",
        "print(\"\\nStarting GPT-2 fine-tune...\\n\")\n",
        "for it in range(max_iters):\n",
        "    # set scheduled LR\n",
        "    lr_now = get_lr(it, base_lr=learning_rate, warmup_iters=warmup_iters,\n",
        "                    lr_decay_iters=lr_decay_iters, min_lr=min_lr)\n",
        "    for pg in optimizer_ft.param_groups:\n",
        "        pg[\"lr\"] = lr_now\n",
        "\n",
        "    if it % eval_interval == 0:\n",
        "        losses = estimate_loss_ft()\n",
        "        print(f\"step {it}: train {losses['train']:.4f}, val {losses['val']:.4f}\")\n",
        "        steps_curve.append(it); train_curve.append(losses[\"train\"]); val_curve.append(losses[\"val\"])\n",
        "\n",
        "    optimizer_ft.zero_grad(set_to_none=True)\n",
        "\n",
        "    # gradient accumulation loop\n",
        "    for _ in range(gradient_accumulation_steps):\n",
        "        xb, yb = get_batch_ft(\"train\", train_ctx)\n",
        "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
        "            _, loss = model_ft(xb, yb)\n",
        "            loss = loss / gradient_accumulation_steps\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "    if grad_clip:\n",
        "        scaler.unscale_(optimizer_ft)\n",
        "        torch.nn.utils.clip_grad_norm_(model_ft.parameters(), grad_clip)\n",
        "\n",
        "    scaler.step(optimizer_ft)\n",
        "    scaler.update()\n",
        "\n",
        "print(\"\\nFine-tune completed!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tc9TYmjntUld",
        "outputId": "2d2f8fec-b8f3-4bcd-f588-15f61515fe79"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting GPT-2 fine-tune...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-661167125.py:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train 4.5327, val 4.5381\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-740941783.py:39: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 100: train 3.4355, val 3.2389\n",
            "step 200: train 3.0350, val 3.4335\n",
            "step 300: train 2.6928, val 3.4208\n",
            "step 400: train 2.4668, val 3.5375\n",
            "step 500: train 2.3807, val 3.8914\n",
            "step 600: train 2.0861, val 3.9738\n",
            "step 700: train 1.7655, val 4.2222\n",
            "step 800: train 1.7733, val 4.3797\n",
            "step 900: train 1.6556, val 4.6913\n",
            "\n",
            "Fine-tune completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y4ZozMATtWTo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}