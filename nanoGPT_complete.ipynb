{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nanoGPT: Complete Implementation\n",
    "\n",
    "This notebook is a complete standalone implementation based on Andrej Karpathy's nanoGPT repository.  \n",
    "**Repository:** https://github.com/karpathy/nanoGPT\n",
    "\n",
    "This notebook contains:\n",
    "1. **Complete GPT model architecture** (faithful to the original)\n",
    "2. **All methods including:**\n",
    "   - `from_pretrained()` - Load pretrained GPT-2 models\n",
    "   - `crop_block_size()` - Model surgery for smaller context\n",
    "   - `estimate_mfu()` - Model FLOPS utilization\n",
    "3. Training loop with proper optimization\n",
    "4. Character-level Shakespeare dataset preparation\n",
    "5. Text generation capabilities\n",
    "6. Examples of fine-tuning pretrained GPT-2\n",
    "\n",
    "The implementation is designed to be educational and hackable while remaining efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "import requests\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Set device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Complete Model Architecture\n",
    "\n",
    "This is the complete, faithful implementation from nanoGPT's model.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False\"\"\"\n",
    "\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"Multi-head masked self-attention layer\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        \n",
    "        # Key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # Output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # Regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        \n",
    "        # Flash attention support (PyTorch >= 2.0)\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            # Causal mask to ensure attention is only applied to the left in the input sequence\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                        .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()  # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # Calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
    "\n",
    "        # Causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            # Efficient attention using Flash Attention CUDA kernels\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, \n",
    "                                                                  dropout_p=self.dropout if self.training else 0, \n",
    "                                                                  is_causal=True)\n",
    "        else:\n",
    "            # Manual implementation of attention\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v  # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        \n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)  # Re-assemble all head outputs side by side\n",
    "\n",
    "        # Output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"Feed-forward network\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu    = nn.GELU()\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block: communication followed by computation\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50304  # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True  # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    \"\"\"GPT Language Model - Complete Implementation\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        \n",
    "        # Weight tying: https://paperswithcode.com/method/weight-tying\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        # Init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # Apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        # Report number of parameters\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        \"\"\"\n",
    "        Return the number of parameters in the model.\n",
    "        For non-embedding count (default), the position embeddings get subtracted.\n",
    "        The token embeddings would too, except due to the parameter sharing these\n",
    "        params are actually used as weights in the final layer, so we include them.\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device)  # shape (t)\n",
    "\n",
    "        # Forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx)  # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos)  # position embeddings of shape (t, n_embd)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            # If we are given some desired targets also calculate the loss\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            # Inference-time mini-optimization: only forward the lm_head on the very last position\n",
    "            logits = self.lm_head(x[:, [-1], :])  # note: using list [-1] to preserve the time dim\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def crop_block_size(self, block_size):\n",
    "        \"\"\"\n",
    "        Model surgery to decrease the block size if necessary.\n",
    "        e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)\n",
    "        but want to use a smaller block size for some smaller, simpler model\n",
    "        \"\"\"\n",
    "        assert block_size <= self.config.block_size\n",
    "        self.config.block_size = block_size\n",
    "        self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])\n",
    "        for block in self.transformer.h:\n",
    "            if hasattr(block.attn, 'bias'):\n",
    "                block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type, override_args=None):\n",
    "        \"\"\"\n",
    "        Load pretrained GPT-2 weights from HuggingFace transformers.\n",
    "        Available models: 'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'\n",
    "        \"\"\"\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        override_args = override_args or {}  # default to empty dict\n",
    "        # only dropout can be overridden see more notes below\n",
    "        assert all(k == 'dropout' for k in override_args)\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        }[model_type]\n",
    "        print(\"forcing vocab_size=50257, block_size=1024, bias=True\")\n",
    "        config_args['vocab_size'] = 50257  # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024  # always 1024 for GPT model checkpoints\n",
    "        config_args['bias'] = True  # always True for GPT model checkpoints\n",
    "        # we can override the dropout rate, if desired\n",
    "        if 'dropout' in override_args:\n",
    "            print(f\"overriding dropout rate to {override_args['dropout']}\")\n",
    "            config_args['dropout'] = override_args['dropout']\n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')]  # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')]  # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')]  # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
    "        # Start with all of the candidate parameters\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        # Filter out those that do not require grad\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # Create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "        # Create AdamW optimizer and use the fused version if it is available\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == 'cuda'\n",
    "        extra_args = dict(fused=True) if use_fused else dict()\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
    "        print(f\"using fused AdamW: {use_fused}\")\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def estimate_mfu(self, fwdbwd_per_iter, dt):\n",
    "        \"\"\"\n",
    "        Estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS.\n",
    "        See PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n",
    "        \"\"\"\n",
    "        # first estimate the number of flops we do per iteration.\n",
    "        N = self.get_num_params()\n",
    "        cfg = self.config\n",
    "        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size\n",
    "        flops_per_token = 6*N + 12*L*H*Q*T\n",
    "        flops_per_fwdbwd = flops_per_token * T\n",
    "        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n",
    "        # express our flops throughput as ratio of A100 bfloat16 peak flops\n",
    "        flops_achieved = flops_per_iter * (1.0/dt)  # per second\n",
    "        flops_promised = 312e12  # A100 GPU bfloat16 peak flops is 312 TFLOPS\n",
    "        mfu = flops_achieved / flops_promised\n",
    "        return mfu\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # If the sequence context is growing too long we must crop it at block_size\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            # Forward the model to get the logits for the index in the sequence\n",
    "            logits, _ = self(idx_cond)\n",
    "            # Pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # Optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            # Apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # Append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download and Prepare Shakespeare Dataset\n",
    "\n",
    "We'll download the complete works of Shakespeare and prepare it for character-level language modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Shakespeare dataset\n",
    "input_file_path = 'input.txt'\n",
    "\n",
    "if not os.path.exists(input_file_path):\n",
    "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "    with open(input_file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(requests.get(data_url).text)\n",
    "\n",
    "with open(input_file_path, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f\"Length of dataset in characters: {len(text):,}\")\n",
    "print(f\"\\nFirst 500 characters:\\n{text[:500]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create character-level vocabulary\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"All characters: {''.join(chars)}\")\n",
    "\n",
    "# Create mappings\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "def encode(s):\n",
    "    return [stoi[c] for c in s]\n",
    "\n",
    "def decode(l):\n",
    "    return ''.join([itos[i] for i in l])\n",
    "\n",
    "# Test encoding/decoding\n",
    "print(f\"\\nExample encoding: {encode('hello')}\")\n",
    "print(f\"Example decoding: {decode(encode('hello'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare train and validation splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "print(f\"Train size: {len(train_data):,} characters\")\n",
    "print(f\"Val size: {len(val_data):,} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. BPE Tokenization (GPT-2 Style)\n",
    "\n",
    "In addition to character-level tokenization, nanoGPT supports BPE (Byte Pair Encoding) tokenization using tiktoken. This is what GPT-2 uses and is essential for working with pretrained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPE Tokenization using tiktoken (for GPT-2)\n",
    "# Install with: pip install tiktoken\n",
    "\n",
    "try:\n",
    "    import tiktoken\n",
    "    \n",
    "    # Initialize GPT-2 BPE encoder\n",
    "    enc = tiktoken.get_encoding(\"gpt2\")\n",
    "    \n",
    "    def encode_bpe(s):\n",
    "        \"\"\"Encode string to BPE token ids\"\"\"\n",
    "        return enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
    "    \n",
    "    def decode_bpe(l):\n",
    "        \"\"\"Decode BPE token ids to string\"\"\"\n",
    "        return enc.decode(l)\n",
    "    \n",
    "    print(\"✓ tiktoken loaded successfully!\")\n",
    "    print(f\"\\nGPT-2 BPE vocab size: {enc.n_vocab}\")\n",
    "    \n",
    "    # Test BPE encoding\n",
    "    test_text = \"Hello, world! This is GPT-2 tokenization.\"\n",
    "    test_tokens = encode_bpe(test_text)\n",
    "    print(f\"\\nExample text: {test_text}\")\n",
    "    print(f\"BPE tokens: {test_tokens}\")\n",
    "    print(f\"Number of tokens: {len(test_tokens)}\")\n",
    "    print(f\"Decoded back: {decode_bpe(test_tokens)}\")\n",
    "    \n",
    "    # Compare with character-level\n",
    "    print(f\"\\nCharacter-level would use {len(test_text)} tokens\")\n",
    "    print(f\"BPE uses {len(test_tokens)} tokens (more efficient!)\")\n",
    "    \n",
    "    bpe_available = True\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"tiktoken not installed. Install with: pip install tiktoken\")\n",
    "    print(\"You can still use character-level tokenization for Shakespeare.\")\n",
    "    bpe_available = False\n",
    "    encode_bpe = None\n",
    "    decode_bpe = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Shakespeare with BPE\n",
    "\n",
    "Let's tokenize the Shakespeare dataset using BPE encoding (like the original nanoGPT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if bpe_available:\n",
    "    # Tokenize Shakespeare data with BPE\n",
    "    shakespeare_bpe_tokens = encode_bpe(text)\n",
    "    print(f\"Shakespeare dataset:\")\n",
    "    print(f\"  Original characters: {len(text):,}\")\n",
    "    print(f\"  BPE tokens: {len(shakespeare_bpe_tokens):,}\")\n",
    "    print(f\"  Compression ratio: {len(text) / len(shakespeare_bpe_tokens):.2f}x\")\n",
    "    \n",
    "    # Create train/val split for BPE tokens\n",
    "    data_bpe = torch.tensor(shakespeare_bpe_tokens, dtype=torch.long)\n",
    "    n_bpe = int(0.9 * len(data_bpe))\n",
    "    train_data_bpe = data_bpe[:n_bpe]\n",
    "    val_data_bpe = data_bpe[n_bpe:]\n",
    "    \n",
    "    print(f\"\\nBPE tokenized splits:\")\n",
    "    print(f\"  Train: {len(train_data_bpe):,} tokens\")\n",
    "    print(f\"  Val: {len(val_data_bpe):,} tokens\")\n",
    "else:\n",
    "    print(\"Skipping BPE tokenization (tiktoken not available)\")\n",
    "    train_data_bpe = None\n",
    "    val_data_bpe = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize Model (Small for Training)\n",
    "\n",
    "For training from scratch on Shakespeare, we'll use a smaller model configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose tokenization method\n",
    "use_bpe = bpe_available and True  # Set to False to force character-level\n",
    "\n",
    "if use_bpe:\n",
    "    print(\"Using BPE tokenization (GPT-2 style)\")\n",
    "    model_vocab_size = 50304  # GPT-2 vocab of 50257, padded to 50304 for efficiency\n",
    "    train_data_active = train_data_bpe\n",
    "    val_data_active = val_data_bpe\n",
    "    encode_fn = encode_bpe\n",
    "    decode_fn = decode_bpe\n",
    "else:\n",
    "    print(\"Using character-level tokenization\")\n",
    "    model_vocab_size = vocab_size\n",
    "    train_data_active = train_data\n",
    "    val_data_active = val_data\n",
    "    encode_fn = encode\n",
    "    decode_fn = decode\n",
    "\n",
    "# Small config for training from scratch on Shakespeare\n",
    "config_small = GPTConfig(\n",
    "    block_size=256,      # Context length\n",
    "    vocab_size=model_vocab_size,\n",
    "    n_layer=6,\n",
    "    n_head=6,\n",
    "    n_embd=384,\n",
    "    dropout=0.2,\n",
    "    bias=False           # False is slightly better and faster\n",
    ")\n",
    "\n",
    "model = GPT(config_small)\n",
    "model.to(device)\n",
    "\n",
    "print(f\"\\nModel configuration:\")\n",
    "print(f\"  Tokenization: {'BPE (GPT-2)' if use_bpe else 'Character-level'}\")\n",
    "print(f\"  Vocab size: {config_small.vocab_size:,}\")\n",
    "print(f\"  Block size (context length): {config_small.block_size}\")\n",
    "print(f\"  Number of layers: {config_small.n_layer}\")\n",
    "print(f\"  Number of heads: {config_small.n_head}\")\n",
    "print(f\"  Embedding dimension: {config_small.n_embd}\")\n",
    "print(f\"  Dropout: {config_small.dropout}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "batch_size = 64\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 200\n",
    "gradient_accumulation_steps = 1\n",
    "grad_clip = 1.0\n",
    "\n",
    "# Learning rate decay settings\n",
    "decay_lr = True\n",
    "warmup_iters = 100\n",
    "lr_decay_iters = max_iters\n",
    "min_lr = 3e-5\n",
    "\n",
    "def get_batch(split):\n",
    "    \"\"\"Generate a small batch of data of inputs x and targets y\"\"\"\n",
    "    data = train_data_active if split == 'train' else val_data_active\n",
    "    ix = torch.randint(len(data) - config_small.block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+config_small.block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+config_small.block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    \"\"\"Estimate the average loss on train and val splits\"\"\"\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "def get_lr(it):\n",
    "    \"\"\"Learning rate decay scheduler (cosine with warmup)\"\"\"\n",
    "    # 1) Linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) If it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3) In between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))  # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer\n",
    "optimizer = model.configure_optimizers(\n",
    "    weight_decay=0.1,\n",
    "    learning_rate=learning_rate,\n",
    "    betas=(0.9, 0.95),\n",
    "    device_type=device\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "print(\"\\nStarting training...\\n\")\n",
    "X, Y = get_batch('train')  # Fetch the first batch\n",
    "t0 = time.time()\n",
    "\n",
    "for iter_num in range(max_iters):\n",
    "    # Determine and set the learning rate for this iteration\n",
    "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    # Evaluate the loss on train/val sets\n",
    "    if iter_num % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        \n",
    "        # Estimate MFU if on GPU\n",
    "        if device == 'cuda' and iter_num > 0:\n",
    "            dt = time.time() - t0\n",
    "            mfu = model.estimate_mfu(batch_size * gradient_accumulation_steps, dt / eval_interval)\n",
    "            print(f\"  MFU: {mfu*100:.2f}%\")\n",
    "            t0 = time.time()\n",
    "\n",
    "    # Forward backward update, with optional gradient accumulation\n",
    "    for micro_step in range(gradient_accumulation_steps):\n",
    "        logits, loss = model(X, Y)\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        X, Y = get_batch('train')\n",
    "        loss.backward()\n",
    "    \n",
    "    # Clip the gradient\n",
    "    if grad_clip != 0.0:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    \n",
    "    # Step the optimizer\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    # Timing and logging\n",
    "    if iter_num % 100 == 0 and iter_num > 0:\n",
    "        t1 = time.time()\n",
    "        dt = t1 - t0\n",
    "        t0 = t1\n",
    "        lossf = loss.item() * gradient_accumulation_steps\n",
    "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms\")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Text Generation from Trained Model\n",
    "\n",
    "Generate text using the trained model with the chosen tokenization scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# Generate from the model\n",
    "context = \"\\n\"  # Start with a newline\n",
    "context_ids = torch.tensor(encode_fn(context), dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "print(f\"Generated text from trained model (using {('BPE' if use_bpe else 'character-level')} tokenization):\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Generate multiple samples\n",
    "num_samples = 3\n",
    "max_new_tokens = 500\n",
    "\n",
    "for i in range(num_samples):\n",
    "    generated_ids = model.generate(context_ids, max_new_tokens=max_new_tokens, temperature=0.8, top_k=200)\n",
    "    generated_text = decode_fn(generated_ids[0].tolist())\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(generated_text)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Load Pretrained GPT-2 Model\n",
    "\n",
    "Now let's demonstrate the `from_pretrained()` method to load a pretrained GPT-2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained GPT-2\n",
    "# Note: This requires the 'transformers' library: pip install transformers\n",
    "# Uncomment the following lines to load a pretrained model\n",
    "\n",
    "# model_gpt2 = GPT.from_pretrained('gpt2')\n",
    "# model_gpt2.to(device)\n",
    "# model_gpt2.eval()\n",
    "\n",
    "print(\"To load a pretrained GPT-2 model, uncomment the lines above.\")\n",
    "print(\"Available models: 'gpt2' (124M), 'gpt2-medium' (350M), 'gpt2-large' (774M), 'gpt2-xl' (1558M)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Generate Text with Pretrained GPT-2\n",
    "\n",
    "Generate text using the pretrained GPT-2 model with BPE tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of generating with pretrained GPT-2\n",
    "if bpe_available:\n",
    "    print(\"Loading pretrained GPT-2 model...\")\n",
    "    print(\"Note: This will download ~500MB on first run\\n\")\n",
    "    \n",
    "    # Uncomment to load and use:\n",
    "    # model_gpt2 = GPT.from_pretrained('gpt2')\n",
    "    # model_gpt2.to(device)\n",
    "    # model_gpt2.eval()\n",
    "    # \n",
    "    # # Generate text\n",
    "    # prompt = \"What is the answer to life, the universe, and everything?\"\n",
    "    # print(f\"Prompt: {prompt}\\n\")\n",
    "    # \n",
    "    # tokens = torch.tensor(encode_bpe(prompt), dtype=torch.long, device=device).unsqueeze(0)\n",
    "    # \n",
    "    # with torch.no_grad():\n",
    "    #     generated = model_gpt2.generate(tokens, max_new_tokens=100, temperature=0.8, top_k=200)\n",
    "    #     generated_text = decode_bpe(generated[0].tolist())\n",
    "    #     print(\"Generated text:\")\n",
    "    #     print(\"=\" * 80)\n",
    "    #     print(generated_text)\n",
    "    #     print(\"=\" * 80)\n",
    "    \n",
    "    print(\"Uncomment the code above to generate text with pretrained GPT-2.\")\n",
    "    print(\"\\nYou can try prompts like:\")\n",
    "    print('  - \"What is the answer to life, the universe, and everything?\"')\n",
    "    print('  - \"Once upon a time\"')\n",
    "    print('  - \"The future of artificial intelligence\"')\n",
    "else:\n",
    "    print(\"tiktoken not available. Install with: pip install tiktoken transformers\")\n",
    "    print(\"Then you can load pretrained GPT-2 models and generate text.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Demonstrate crop_block_size()\n",
    "\n",
    "Show how to reduce the context length of a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of cropping block size\n",
    "# Useful if you load a pretrained model with block_size=1024 but want to use 256\n",
    "\n",
    "print(f\"Current block size: {model.config.block_size}\")\n",
    "print(f\"Current position embedding shape: {model.transformer.wpe.weight.shape}\")\n",
    "\n",
    "# If we wanted to crop to 128 tokens (uncomment to try):\n",
    "# model.crop_block_size(128)\n",
    "# print(f\"\\nAfter cropping:\")\n",
    "# print(f\"New block size: {model.config.block_size}\")\n",
    "# print(f\"New position embedding shape: {model.transformer.wpe.weight.shape}\")\n",
    "\n",
    "print(\"\\nThis is useful when you want to use a smaller context window than the original model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Fine-tuning Pretrained GPT-2 on Shakespeare\n",
    "\n",
    "Example of how to fine-tune a pretrained GPT-2 model on your custom dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning example (uncomment to run)\n",
    "# This demonstrates the complete workflow:\n",
    "# 1. Load pretrained GPT-2\n",
    "# 2. Prepare data with BPE tokenization\n",
    "# 3. Fine-tune with lower learning rate\n",
    "\n",
    "print(\"\"\"To fine-tune GPT-2 on Shakespeare:\n",
    "\n",
    "1. Install dependencies:\n",
    "   pip install transformers tiktoken\n",
    "\n",
    "2. Load pretrained model:\n",
    "   model = GPT.from_pretrained('gpt2')\n",
    "\n",
    "3. Tokenize your data with tiktoken (BPE):\n",
    "   import tiktoken\n",
    "   enc = tiktoken.get_encoding('gpt2')\n",
    "   tokens = enc.encode(text)\n",
    "\n",
    "4. Train with smaller learning rate:\n",
    "   learning_rate = 6e-5  # Lower than from-scratch training\n",
    "   max_iters = 1000      # Fewer iterations needed\n",
    "\n",
    "5. The fine-tuned model will generate Shakespeare-like text\n",
    "   but with the knowledge from GPT-2's pretraining!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model checkpoint (nanoGPT format)\n",
    "checkpoint = {\n",
    "    'model': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'model_args': {\n",
    "        'n_layer': config_small.n_layer,\n",
    "        'n_head': config_small.n_head,\n",
    "        'n_embd': config_small.n_embd,\n",
    "        'block_size': config_small.block_size,\n",
    "        'bias': config_small.bias,\n",
    "        'vocab_size': config_small.vocab_size,\n",
    "        'dropout': config_small.dropout,\n",
    "    },\n",
    "    'iter_num': max_iters,\n",
    "    'best_val_loss': estimate_loss()['val'],\n",
    "    'config': {\n",
    "        'dataset': 'shakespeare',\n",
    "        'tokenization': 'bpe' if use_bpe else 'char',\n",
    "    },\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, 'ckpt.pt')\n",
    "print(\"Model checkpoint saved to 'ckpt.pt'\")\n",
    "\n",
    "# Save tokenization metadata\n",
    "if use_bpe:\n",
    "    # For BPE, we don't need to save encoding (tiktoken handles it)\n",
    "    print(\"Using BPE tokenization (tiktoken)\")\n",
    "else:\n",
    "    # For character-level, save the character mappings\n",
    "    meta = {\n",
    "        'vocab_size': vocab_size,\n",
    "        'itos': itos,\n",
    "        'stoi': stoi,\n",
    "    }\n",
    "    with open('meta.pkl', 'wb') as f:\n",
    "        pickle.dump(meta, f)\n",
    "    print(\"Character mappings saved to 'meta.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model checkpoint (compatible with nanoGPT sample.py)\n",
    "def load_checkpoint(checkpoint_path='ckpt.pt', device='cuda'):\n",
    "    \"\"\"\n",
    "    Load a model checkpoint in the same way as nanoGPT's sample.py\n",
    "    Returns: model, encode_fn, decode_fn\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    # Create model from checkpoint\n",
    "    gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "    model = GPT(gptconf)\n",
    "    \n",
    "    # Load state dict\n",
    "    state_dict = checkpoint['model']\n",
    "    # Handle compiled models (remove '_orig_mod.' prefix)\n",
    "    unwanted_prefix = '_orig_mod.'\n",
    "    for k, v in list(state_dict.items()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "    \n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Determine tokenization scheme\n",
    "    tokenization = checkpoint.get('config', {}).get('tokenization', 'char')\n",
    "    \n",
    "    if tokenization == 'bpe':\n",
    "        print(\"Loading BPE tokenization (tiktoken)\")\n",
    "        if not bpe_available:\n",
    "            raise ImportError(\"tiktoken required for BPE. Install with: pip install tiktoken\")\n",
    "        encode_fn = encode_bpe\n",
    "        decode_fn = decode_bpe\n",
    "    else:\n",
    "        print(\"Loading character-level tokenization\")\n",
    "        # Load meta.pkl for character mappings\n",
    "        with open('meta.pkl', 'rb') as f:\n",
    "            meta = pickle.load(f)\n",
    "        stoi_loaded = meta['stoi']\n",
    "        itos_loaded = meta['itos']\n",
    "        encode_fn = lambda s: [stoi_loaded[c] for c in s]\n",
    "        decode_fn = lambda l: ''.join([itos_loaded[i] for i in l])\n",
    "    \n",
    "    return model, encode_fn, decode_fn\n",
    "\n",
    "# Example usage:\n",
    "# loaded_model, encode_fn, decode_fn = load_checkpoint('ckpt.pt')\n",
    "# prompt = \"ROMEO:\"\n",
    "# tokens = torch.tensor(encode_fn(prompt), dtype=torch.long, device=device).unsqueeze(0)\n",
    "# with torch.no_grad():\n",
    "#     generated = loaded_model.generate(tokens, max_new_tokens=200)\n",
    "#     print(decode_fn(generated[0].tolist()))\n",
    "\n",
    "print(\"\\nUse load_checkpoint() to reload a saved model with the correct tokenization.\")\n",
    "print(\"This is compatible with nanoGPT's checkpoint format!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze model parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"Model Statistics:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Non-embedding parameters: {model.get_num_params():,}\")\n",
    "print(f\"\\nLayer breakdown:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"  {name:50s} {str(list(param.shape)):20s} {param.numel():10,} params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation\n",
    "print(\"\\nFinal evaluation:\")\n",
    "losses = estimate_loss()\n",
    "print(f\"  Train loss: {losses['train']:.4f}\")\n",
    "print(f\"  Val loss: {losses['val']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Summary of All Features\n",
    "\n",
    "This notebook now includes all methods from the original nanoGPT `model.py` plus proper tokenization support:\n",
    "\n",
    "### Core Methods:\n",
    "- ✅ `__init__()` - Initialize the model\n",
    "- ✅ `forward()` - Forward pass with loss calculation\n",
    "- ✅ `generate()` - Autoregressive text generation\n",
    "- ✅ `configure_optimizers()` - Set up AdamW with weight decay\n",
    "\n",
    "### Advanced Methods:\n",
    "- ✅ `from_pretrained()` - Load pretrained GPT-2 weights (gpt2, gpt2-medium, gpt2-large, gpt2-xl)\n",
    "- ✅ `crop_block_size()` - Reduce context length for smaller models\n",
    "- ✅ `estimate_mfu()` - Calculate model FLOPS utilization on A100 GPUs\n",
    "- ✅ `get_num_params()` - Count model parameters\n",
    "- ✅ `_init_weights()` - Weight initialization\n",
    "\n",
    "### Tokenization Support:\n",
    "- ✅ **Character-level tokenization** - For simple datasets like Shakespeare\n",
    "- ✅ **BPE tokenization (tiktoken)** - GPT-2 style, for pretrained models\n",
    "- ✅ Automatic tokenization detection when loading checkpoints\n",
    "- ✅ Compatible with nanoGPT's checkpoint format\n",
    "\n",
    "### Key Features:\n",
    "1. **Train from scratch** on character-level Shakespeare\n",
    "2. **Train with BPE** tokenization (like GPT-2)\n",
    "3. **Load pretrained GPT-2** and generate text immediately\n",
    "4. **Fine-tune GPT-2** on custom datasets\n",
    "5. **Adjust context length** dynamically with crop_block_size()\n",
    "6. **Monitor efficiency** with MFU estimation\n",
    "7. **Save/load models** in nanoGPT-compatible format\n",
    "\n",
    "### Tokenization Comparison:\n",
    "```python\n",
    "# Character-level (simple, but inefficient)\n",
    "text = \"Hello, world!\"\n",
    "char_tokens = [stoi[c] for c in text]  # 13 tokens\n",
    "\n",
    "# BPE (efficient, used by GPT-2)\n",
    "bpe_tokens = encode_bpe(text)  # 4 tokens: [15496, 11, 995, 0]\n",
    "```\n",
    "\n",
    "### Experiment Ideas:\n",
    "- Compare character-level vs BPE tokenization on Shakespeare\n",
    "- Load GPT-2 and generate text on various prompts\n",
    "- Fine-tune GPT-2 on Shakespeare with BPE tokenization\n",
    "- Try different model sizes (gpt2 through gpt2-xl)\n",
    "- Experiment with different context lengths using crop_block_size()\n",
    "- Profile training efficiency with estimate_mfu()\n",
    "- Train a small model with BPE to see the difference from character-level\n",
    "\n",
    "### Dependencies:\n",
    "```bash\n",
    "pip install torch numpy\n",
    "pip install tiktoken          # For BPE tokenization\n",
    "pip install transformers       # For loading pretrained GPT-2\n",
    "pip install requests          # For downloading datasets\n",
    "```\n",
    "\n",
    "For more information, visit: https://github.com/karpathy/nanoGPT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
